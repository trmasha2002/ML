{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Предобработка текстов - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Правильная предобработка текста позволяет добиться:\n",
    "* улучшения получаемых результатов\n",
    "* ускорения экспериментов\n",
    "* воспроизводимости экспериментов\n",
    "* удобной интерпретации и презентации результатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наивные методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = '''«Тинькофф Банк» — российский коммерческий банк, сфокусированный полностью на дистанционном обслуживании\n",
    ", не имеющий розничных отделений. Штаб-квартира банка расположена в Москве.'''\n",
    "sent.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent.split()[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регулярные выражение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулярное выражение — это последовательность символов, используемая для поиска и замены текста в строке или файле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* поиска в строке;\n",
    "* разбиения строки на подстроки;\n",
    "* замены части строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='img/regular-expressions-cheat-sheet-v2.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.match(r'Tinkoff', 'Tinkoff junior the best')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.search(r'Tinkoff', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
    "result.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.findall(r'Tinkoff', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.split(r'the', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = re.sub(r'Bank', 'World', 'Tinkoff Junior the Best Team Tinkoff Bank')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно создавать паттерны\n",
    "pattern = re.compile('Tinkoff')\n",
    "result = pattern.findall('Tinkoff Junior the Best Team Tinkoff Bank')\n",
    "print(result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Разделите используя знаки пунктуации\n",
    "line = 'asdf fjdk;afed,fjek,asdf,foo' \n",
    "#  Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Оставить в тексте только русские буквы\n",
    "sent = '''Ночь закрытых дверей!!! Штаб-квартира Tinkoff.ru 27 ноября'''\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Извлечь домены, заменить все домены на tinkoff.ru\n",
    "string = 'abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Больше примеров регулярных выражений: https://regex101.com/r/nG1gU7/27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Спелл чекеры - проверка правописания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Левенштейн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Расстояние Левенштейна (также редакционное расстояние или дистанция редактирования) между двумя строками в теории информации и компьютерной лингвистике — это минимальное количество операций вставки одного символа, удаления одного символа и замены одного символа на другой, необходимых для превращения одной строки в другую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возможная самостоятельная работа: Реализовать алгоритм левенштейна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/pythoьn-Levenshtein/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Levenshtein.distance('привет', 'приве')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возможная самостоятельная работа: Реализовать алгоритм "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Домашняя работа:\n",
    "1. Реализовать алгоритм,проверяющий корректность написания страны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Уменьшение словаря\n",
    "* Плохие слова:\n",
    "* Слишком частые \n",
    "  * русский язык: и, но, я, ты, ... \n",
    "  * английский язык: a, the, I, ... \n",
    "  * специфичные для коллекции: \"сообщать\" в новостях\n",
    "* Слишком редкие\n",
    "* Стоп-слова \n",
    "  *Предлоги, междометия, частицы, цифры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nltk.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_eng = set(stopwords.words('english'))\n",
    "list(sw_eng)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sw_ru = set(stopwords.words('russian'))\n",
    "list(sw_ru)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Наступило молчание. Графиня глядела на гостью, приятно улыбаясь, впрочем, не скрывая \\\n",
    "того, что не огорчится теперь нисколько, если гостья поднимется и уедет.\\\n",
    "Дочь гостьи уже оправляла платье, вопросительно глядя на мать, как вдруг из\\\n",
    "соседней комнаты послышался бег к двери нескольких мужских и женских ног,\\\n",
    "грохот зацепленного и поваленного стула, и в комнату вбежала тринадцатилетняя девочка,\\\n",
    "запахнув что-то короткою кисейною юбкою, и остановилась посередине комнаты. Очевидно было,\\\n",
    "она нечаянно, с нерассчитанного бега, заскочила так далеко. В дверях в ту же минуту показались\\\n",
    "студент с малиновым воротником, гвардейский офицер, пятнадцатилетняя девочка и толстый румяный\\\n",
    "мальчик в детской курточке.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Отчистить текст от стоп слов\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация\n",
    "разделение на токены, элементарные единицы текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_string = 'Привет, какая у меня полная сумма задолженности по кредитной карте'\n",
    "light_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_string = 'Ой, у вас несколько кредитных карт, выберите, пожалуйста, одну и введите ее номер'\n",
    "hard_string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hard_string = 'Привет! Ты видел мр.Смита сегодня утром?'\n",
    "expr = r'[^(\\w.\\w)\\w\\s]'\n",
    "parser=re.compile(expr)\n",
    "tmp_string = parser.sub(r'', hard_string)\n",
    "print(tmp_string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Разделить строку на предложения\n",
    "hard_string = 'Привет. Ты видел мр.Смита сегодня утром?'\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_string = 'Привет. Ты видел мр.Смита сегодня утром?'\n",
    "exp = r'(?<!\\w\\.\\w.)(?<![А-Я][а-я]\\.)(?<=\\.|\\?)\\s'\n",
    "tmp_string = re.split(exp, hard_string)\n",
    "print(tmp_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг\n",
    "СТЕММИНГ - нормализация слов путем отбрасывания окончаний (согласно правилам, основанным на грамматике языка)\n",
    "* Стеммеры (nltk)\n",
    "    * Porter stemmer\n",
    "    * Snowball stemmer\n",
    "    * Lancaster stemmer\n",
    "    * MyStem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стемминг\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "sent = 'George admitted the talks happened'\n",
    "print(' '.join([stemmer.stem(word) for word in sent.split()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стемминг\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "sent = 'write wrote written'\n",
    "print(' '.join([stemmer.stem(word) for word in sent.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стемминг\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='russian')\n",
    "sent = 'Опрошенные считают налоги необходимыми'\n",
    "print(' '.join([stemmer.stem(word) for word in sent.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'поле пол полка полк'\n",
    "print(' '.join([stemmer.stem(word) for word in sent.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'крутой крутейший крутить'\n",
    "print(' '.join([stemmer.stem(word) for word in sent.split()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация - приведение слов к начальной морфологической форме (с помощью словаря и грамматики языка)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизаторы\n",
    "* pymorphy2 (язык русский, украинский)\n",
    "* mystem3 (язык русский)\n",
    "* Wordnet Lemmatizer (NLTK, язык английский, требует POS метку)\n",
    "* Metaphraz (язык русский)\n",
    "* Coda/Cadenza (языки русский и английский)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизатор на самом деле довольно сложный, ему нужны теги частей речи (POS).\n",
    "\n",
    "По умолчанию функция WordNetLemmatizer.lemmatize () будет считать, что это слово является существительным, если на входе не обнаружен тег POS.\n",
    "\n",
    "Сначала вам понадобится функция pos_tag, чтобы пометить предложение и использовать тег, чтобы преобразовать его в теги WordNet, а затем передать его в WordNetLemmatizer.\n",
    "\n",
    "Примечание. Лемматизация не будет работать только на одиночных словах без контекста или знании своего тега POS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательная задача\n",
    "from nltk import wordnet, pos_tag\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    my_switch = {\n",
    "        'J': wordnet.wordnet.ADJ,\n",
    "        'V': wordnet.wordnet.VERB,\n",
    "        'N': wordnet.wordnet.NOUN,\n",
    "        'R': wordnet.wordnet.ADV,\n",
    "    }\n",
    "    for key, item in my_switch.items():\n",
    "        if treebank_tag.startswith(key):\n",
    "            return item\n",
    "    return wordnet.wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'George admitted the talks happened'.split()\n",
    "pos_tagged = pos_tag(sent)\n",
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([get_wordnet_pos(tag) for word, tag in pos_tagged])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "def my_lemmatizer(sent):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_sent = sent.split()\n",
    "    pos_tagged = [(word, get_wordnet_pos(tag))\n",
    "                 for word, tag in pos_tag(tokenized_sent)]\n",
    "    return ' '.join([lemmatizer.lemmatize(word, tag)\n",
    "                    for word, tag in pos_tagged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'George admitted the talks happened'\n",
    "my_lemmatizer(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'write wrote written'\n",
    "my_lemmatizer(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "def my_lemmatizer_ru(sent):\n",
    "    lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "    tokenized_sent = sent.split()\n",
    "    return ' '.join([lemmatizer.parse(word)[0].normal_form\n",
    "                    for word in tokenized_sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Опрошенные считают налоги необходимыми'\n",
    "my_lemmatizer_ru(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'поле пол полка полк'\n",
    "my_lemmatizer_ru(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'крутой крутейший крутить'\n",
    "print(my_lemmatizer_ru(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Стемминг *\n",
    "* Плохо работает для русского языка\n",
    "* Нормально работает для английского\n",
    "* Повышает качество модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Лемматизация*\n",
    "* Лучше стемминга для русского языка\n",
    "* Хорошо работает для английского языка\n",
    "* Повышает качество модели\n",
    "* Гораздо медленнее чем стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Представление словаря в виде бинарных векторов, у которых все значения равны 0, кроме одного, отвечающего за соответствующее слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "corpus = ['Кредитная', 'Дебетовая', 'All Airlines', 'Bravo', 'All games']\n",
    "label_encoder = LabelEncoder()\n",
    "corpus_encoded = label_encoder.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "corpus_encoded = corpus_encoded.reshape(len(corpus_encoded), 1)\n",
    "corpus_onehot_encoded = onehot_encoder.fit_transform(corpus_encoded)\n",
    "print(corpus_onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF (term frequency — частота слова) — отношение числа вхождений некоторого слова к общему числу слов документа. Таким образом, оценивается важность слова в пределах отдельного документа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF (inverse document frequency — обратная частота документа) — инверсия частоты, с которой некоторое слово встречается в документах коллекции. Основоположником данной концепции является Карен Спарк Джонс. Учёт IDF уменьшает вес широкоупотребительных слов. Для кmаждого уникального слова в пределах конкретной коллекции документов существует только одно значение IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "idf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "Y = idf_vectorizer.fit_transform(corpus)\n",
    "print(idf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Близость"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто возникает задач найти близкие друг к другу предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = (\" \".join(df.about.values)).split()\n",
    "unigrams_counter = Counter(words)\n",
    "bigrams = ngrams(words, 2)\n",
    "bigrams_counter = Counter(bigrams)\n",
    "\n",
    "three_chars = ngrams((\" \".join(df.about.values)), 3)\n",
    "three_chars_counter = Counter(three_chars)\n",
    "\n",
    "print('Количество би-грам: {0}'.format(len(bigrams_counter.most_common())))\n",
    "print('Количество уни-грам: {0}'.format(len(unigrams_counter.most_common())))\n",
    "print('Количество буквенных три-грамм: {0}'.format(len(three_chars_counter.most_common())))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: найти близость между предложениями, используя триграммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Го что нибудь порешаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                                 categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Самостоятельная работа: попробовать другие преобразования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пицца"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json') as fin:\n",
    "    trainjson = json.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainjson[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('UID:\\t', trainjson[0]['request_id'], '\\n')\n",
    "print('Title:\\t', trainjson[0]['request_title'], '\\n')\n",
    "print('Text:\\t', trainjson[0]['request_text_edit_aware'], '\\n')\n",
    "print('Tag:\\t', trainjson[0]['requester_received_pizza'], end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.io.json.json_normalize(trainjson) # Pandas magic... \n",
    "df_train = df[['request_id', 'request_title', \n",
    "               'request_text_edit_aware', \n",
    "               'requester_received_pizza']]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "train, valid = train_test_split(df_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    morphy_tag = {'NN':'n',\n",
    "                  'JJ':'a',\n",
    "                  'VB':'v',\n",
    "                  'RB':'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' \n",
    "    \n",
    "def lemmatize_sent(text): \n",
    "    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]\n",
    "\n",
    "lemmatize_sent('He is walking to school')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Input: str, i.e. document/sentence\n",
    "    # Output: list(str) , i.e. list of lemmas\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stopwords_en\n",
    "            and not word.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
